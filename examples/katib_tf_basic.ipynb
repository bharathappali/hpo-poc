{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!pip install kubeflow-katib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!pip install tensorflow==2.9.1"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "97949db8970609d2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def train_mnist_model(parameters):\n",
    "    import tensorflow as tf\n",
    "    import numpy as np\n",
    "    import logging\n",
    "\n",
    "    logging.basicConfig(\n",
    "        format=\"%(asctime)s %(levelname)-8s %(message)s\",\n",
    "        datefmt=\"%Y-%m-%dT%H:%M:%SZ\",\n",
    "        level=logging.INFO,\n",
    "    )\n",
    "    logging.info(\"--------------------------------------------------------------------------------------\")\n",
    "    logging.info(f\"Input Parameters: {parameters}\")\n",
    "    logging.info(\"--------------------------------------------------------------------------------------\\n\\n\")\n",
    "\n",
    "\n",
    "    # Get HyperParameters from the input params dict.\n",
    "    lr = float(parameters[\"lr\"])\n",
    "    num_epoch = int(parameters[\"num_epoch\"])\n",
    "\n",
    "    # Set dist parameters and strategy.\n",
    "    is_dist = parameters[\"is_dist\"]\n",
    "    num_workers = parameters[\"num_workers\"]\n",
    "    batch_size_per_worker = 64\n",
    "    batch_size_global = batch_size_per_worker * num_workers\n",
    "    strategy = tf.distribute.MultiWorkerMirroredStrategy(\n",
    "        communication_options=tf.distribute.experimental.CommunicationOptions(\n",
    "            implementation=tf.distribute.experimental.CollectiveCommunication.RING\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Callback class for logging training.\n",
    "    # Katib parses metrics in this format: <metric-name>=<metric-value>.\n",
    "    class CustomCallback(tf.keras.callbacks.Callback):\n",
    "        def on_epoch_end(self, epoch, logs=None):\n",
    "            logging.info(\n",
    "                \"Epoch {}/{}. accuracy={:.4f} - loss={:.4f}\".format(\n",
    "                    epoch+1, num_epoch, logs[\"accuracy\"], logs[\"loss\"]\n",
    "                )\n",
    "            )\n",
    "\n",
    "    # Prepare MNIST Dataset.\n",
    "    def mnist_dataset(batch_size):\n",
    "        (x_train, y_train), _ = tf.keras.datasets.mnist.load_data()\n",
    "        x_train = x_train / np.float32(255)\n",
    "        y_train = y_train.astype(np.int64)\n",
    "        train_dataset = (\n",
    "            tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "            .shuffle(60000)\n",
    "            .repeat()\n",
    "            .batch(batch_size)\n",
    "        )\n",
    "        return train_dataset\n",
    "\n",
    "    # Build and compile CNN Model.\n",
    "    def build_and_compile_cnn_model():\n",
    "        model = tf.keras.Sequential(\n",
    "            [\n",
    "                tf.keras.layers.InputLayer(input_shape=(28, 28)),\n",
    "                tf.keras.layers.Reshape(target_shape=(28, 28, 1)),\n",
    "                tf.keras.layers.Conv2D(32, 3, activation=\"relu\"),\n",
    "                tf.keras.layers.Flatten(),\n",
    "                tf.keras.layers.Dense(128, activation=\"relu\"),\n",
    "                tf.keras.layers.Dense(10),\n",
    "            ]\n",
    "        )\n",
    "        model.compile(\n",
    "            loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "            optimizer=tf.keras.optimizers.SGD(learning_rate=lr),\n",
    "            metrics=[\"accuracy\"],\n",
    "        )\n",
    "        return model\n",
    "    \n",
    "    # Download Dataset.\n",
    "    dataset = mnist_dataset(batch_size_global)\n",
    "\n",
    "    # For dist strategy we should build model under scope().\n",
    "    if is_dist:\n",
    "        logging.info(\"Running Distributed Training\")\n",
    "        logging.info(\"--------------------------------------------------------------------------------------\\n\\n\")\n",
    "        with strategy.scope():\n",
    "            model = build_and_compile_cnn_model()\n",
    "    else:\n",
    "        logging.info(\"Running Single Worker Training\")\n",
    "        logging.info(\"--------------------------------------------------------------------------------------\\n\\n\")\n",
    "        model = build_and_compile_cnn_model()\n",
    "    \n",
    "    # Start Training.\n",
    "    model.fit(\n",
    "        dataset,\n",
    "        epochs=num_epoch,\n",
    "        steps_per_epoch=70,\n",
    "        callbacks=[CustomCallback()],\n",
    "        verbose=0,\n",
    "    )\n",
    "    \n",
    "import kubeflow.katib as katib\n",
    "\n",
    "# Set parameters with their distribution for HyperParameter Tuning with Katib.\n",
    "parameters = {\n",
    "    \"lr\": katib.search.double(min=0.1, max=0.2),\n",
    "    \"num_epoch\": katib.search.int(min=10, max=15),\n",
    "    \"is_dist\": False,\n",
    "    \"num_workers\": 1\n",
    "}\n",
    "\n",
    "# Start the Katib Experiment.\n",
    "exp_name = \"tune-mnist\"\n",
    "katib_client = katib.KatibClient()\n",
    "\n",
    "katib_client.tune(\n",
    "    name=exp_name,\n",
    "    objective=train_mnist_model,\n",
    "    parameters=parameters,\n",
    "    algorithm_name=\"cmaes\",\n",
    "    objective_metric_name=\"accuracy\",\n",
    "    additional_metric_names=[\"loss\"],\n",
    "    max_trial_count=12,\n",
    "    parallel_trial_count=2,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e0d34837f3f257a4"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "status = katib_client.is_experiment_succeeded(exp_name)\n",
    "print(f\"Katib Experiment is Succeeded: {status}\\n\")\n",
    "\n",
    "best_hps = katib_client.get_optimal_hyperparameters(exp_name)\n",
    "\n",
    "if best_hps != None:\n",
    "    print(\"Current Optimal Trial\\n\")\n",
    "    print(best_hps)\n",
    "    \n",
    "    for hp in best_hps.parameter_assignments:\n",
    "        if hp.name == \"lr\":\n",
    "            best_lr = hp.value\n",
    "            print(f\"Best LR: {best_lr}\")\n",
    "        else:\n",
    "            best_num_epoch = hp.value\n",
    "            print(f\"Best Num Epochs: {best_num_epoch}\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3af960042a64550e"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
